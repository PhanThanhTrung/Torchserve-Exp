# Model startup configurations
load_models=flag_llm_reranker.mar,bge_m3_embedding.mar
model_store=./weights

# Model listening address and port
inference_address=http://127.0.0.1:8080
management_address=http://127.0.0.1:8081
metrics_address=http://127.0.0.1:8082

# gRPC configurations (default settings)
grpc_inference_address=127.0.0.1
grpc_inference_port=7070
grpc_management_address=127.0.0.1
grpc_management_port=7071

# Configure ssl
# private_key_file=mykey.key
# certificate_file=mycert.pem

# Configure max connection age (milliseconds)
# grpc_inference_max_connection_age_ms=Infinite
# grpc_management_max_connection_age_ms=Infinite

# Configure max connection age grace (milliseconds)
# grpc_inference_max_connection_age_grace_ms=Infinite
# grpc_management_max_connection_age_grace_ms=Infinite

# cors_allowed_origin is required to enable CORS, use '*' or your domain name
# cors_allowed_origin=https://yourdomain.com

# Required if you want to use preflight request
# cors_allowed_methods=GET, POST, PUT, OPTIONS

# Required if the request has an Access-Control-Request-Headers header
# cors_allowed_headers=X-Custom-Header

# Limit GPU Usage
number_of_gpu=0

# Configure models
models={\
    "flag_llm_reranker": {\
        "1.0": {\
            "minWorkers": 2,\
            "maxWorkers": 4,\
            "batchSize": 1,\
            "maxBatchDelay": 100,\
            "defaultVersion": true,\
            "marName": "flag_llm_reranker.mar"\
        }\
    },\
    "bge_m3_embedding": {\
        "1.0": {\
            "minWorkers": 1,\
            "maxWorkers": 1,\
            "batchSize": 1,\
            "maxBatchDelay": 100,\
            "defaultVersion": true,\
            "marName": "bge_m3_embedding.mar"\
        }\
    },\
    "llama_cpp_binding": {\
        "1.0": {\
            "minWorkers": 1,\
            "maxWorkers": 1,\
            "batchSize": 1,\
            "maxBatchDelay": 1000,\
            "defaultVersion": true,\
            "marName": "llama_cpp_binding.mar"\
        }\
    }\
}\

# # Configure workers
# default_workers_per_model=1
number_of_netty_threads=32
job_queue_size=1000
model_store=/home/model-server/model-store
workflow_store=/home/model-server/wf-store
